This repository contains our implementation of the "Residual Attention Network for Image Classification" paper along with the project report. To run the code, simply execute the jupyter notebooks in the associated direcotries. 

Submission by Aman Anand(aa4821), Vasanth Margabandhu(vm2656) and Bhavin Dhedhi(bdd2115)

Drive link to the trained models : https://drive.google.com/drive/folders/1af79537iF683vbbh5dGNSUg8xDQ8bNHR

The folder /CIFAR-10 contains the implementation of attention network on the CIFAR-10 dataset with batch size of 64 and 32.

The foler /CIFAR-100 contains the implementation of attention network on the CIFAR-100 dataset with batch size of 64 and 32.

The folder /ImageNet contains the implementation of attention network on the Tiny-ImageNet dataset.

The folder /table contains the table comparing the performance of different network architectures on the datasets.

The folder /architecture contains the architecture of the attention module and the soft mask branch.

# Project tree

 * [CIFAR-10](./CIFAR-10)
   * [cifar_10_batch_size_32.ipynb](./CIFAR-10/cifar_10_batch_size_32.ipynb)
   * [cifar_10_batch_size_64.ipynb](./CIFAR-10/cifar_10_batch_size_64.ipynb)
   
 * [CIFAR-100](./CIFAR-100)
   * [cifar_100_batch_size_32.ipynb](./CIFAR-100/cifar_100_batch_size_32.ipynb)
   * [cifar_100_batch_size_64.ipynb](./CIFAR-100/cifar_100_batch_size_64.ipynb)
   
 * [ImageNet](./ImageNet)
   * [tiny_imagenet.ipynb](./ImageNet/tiny_imagenet.ipynb)
   
 * [table](./table)
   * [Table.PNG](./table/Table.PNG)
   
 * [architecture](./architecture)
   * [attention_module_block.PNG](./architecture/attention_module_block.PNG)
   * [mask_branch.PNG](./architecture/mask_branch.PNG)
 
 * [E4040.2021FALL.YOYO.report.aa4821.bdd2115.vm2656.docx](./E4040.2021FALL.YOYO.report.aa4821.bdd2115.vm2656.docx)
